{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b98ca1d0-0faf-4747-85c2-d48036c7ed0f",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/58568400/weighted-summation-of-embeddings-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e6d7ba6-1ec8-402e-859d-cb57f2c34011",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6af02598-3a9f-4abb-b23a-9c7e88f39920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d973f3-6556-48aa-b774-40d949090fb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Variable multi-categorica\n",
    "\n",
    "Es una variable categorica comun con la diferencia que la columna en la observacion no tiene un solo valor sino una lista de valores.\n",
    "\n",
    "Ej.: Las peliculas pertenecen a varios generos. El genero es una variable categorica comun pero las peliculas pueden petenecer mas de un genero. Entonces una observacion de una pelicula podria ser:\n",
    "\n",
    "- Nombre: Toy Story\n",
    "- Generos: [Comedia, Fantasticas, Aventura]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bca2a64-aa99-42ae-a316-cde9137af3e3",
   "metadata": {},
   "source": [
    "## Embedding Bag\n",
    "\n",
    "* Un Embedding bag permite sumar, promediar(pesado o normal) o quedarnos con el maximo de una lista de embedding vectors.\n",
    "* Son muy usados cuando tenemos una variable muti-categorica.\n",
    "* Si queres usar un promedio pesado el problema es que los pesos no son parametros a aprender, si no que hay que pasarlos. Son parametros fijos :(.\n",
    "* Lo mejor seria tener un EmbeddingBag que aprenda esos pesos ajustandolos en el proceso de backpropagation ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed18fe8-d178-4111-9b17-9ba18e29fc30",
   "metadata": {},
   "source": [
    "## Weighted Mean Embedding Bag\n",
    "\n",
    "* A continuación se implementa un EmbeddingBag con promedio pesado, donde los pesos son parametros \n",
    "a apender por la capa (Módulo en pytorch).\n",
    "* Se separa el problema es dos pasos. \n",
    "  * Una capa embedding comun que en base a los indices de las categorias devueve embedding vectors\n",
    "  * Otra capa (**LinearWeightedAVG**) que toma estos vectores, hace el promedio pesado y se queda con un unico vector embedding promedio para cada batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9d8b3e5-888a-4cb3-858f-239ba369553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_layer(embeding_count):\n",
    "    return nn.Conv1d(in_channels=embeding_count, out_channels=1, kernel_size=1)\n",
    "\n",
    "class LinearWeightedMean(nn.Module):\n",
    "    def __init__(self): \n",
    "        super(LinearWeightedMean, self).__init__()\n",
    "        self._avg = None\n",
    "\n",
    "    def forward(self, x): \n",
    "        if self._avg == None: self._avg = avg_layer(embeding_count = embed.size()[-2])\n",
    "        return torch.stack([self._avg(batch) for batch in embed]).squeeze(-2)\n",
    "\n",
    "class WeightedMeanEmbeddingBag(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super(WeightedMeanEmbeddingBag, self).__init__()\n",
    "        self._emb = embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self._avg = LinearWeightedMean()\n",
    " \n",
    "    def forward(self, x): return self._avg(self._emb(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef685bb2-50c5-469c-a810-690d2b54e596",
   "metadata": {},
   "source": [
    "## Ejemplo\n",
    "\n",
    "* Cada observación es una lista de categorias de una variable categorica codificadas en numeros.\n",
    "* La variable categorica tiene 3 posibles valores, excluyentes en cada posición de la lista de valores. Por ej.: una pelicula no puede tener dos veces el genero comedia.\n",
    "* Cada observacion es una lista de tamaño 3, por que una pelicula podria tener todos los generos posibles (3 en este ejemplo).\n",
    "* Algunas peliculas pueden tener menos generos que el total. Los que faltantes quedan en cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3b6ebc9-cb8e-47fb-b656-360269f0dc61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.2157,  0.9606],\n",
       "        [-0.5736,  0.7405],\n",
       "        [-0.7917, -0.0940],\n",
       "        [ 1.0521, -1.1056]], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = nn.Embedding(\n",
    "    num_embeddings=4, # La opcion sin genero es un valor mas de la categorica.\n",
    "    embedding_dim=2\n",
    ")\n",
    "embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9f8f925-ad6f-4608-b3b9-d658fb566134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = torch.LongTensor([\n",
    "    [ \n",
    "        [1, 2, 3], # La pelicula 1 tiene los generos 1,2 y 3.\n",
    "        [3, 0, 0]  # La pelicula 2 tiene el generos 2 solamente.\n",
    "    ],\n",
    "    [ \n",
    "        [1, 2, 0], \n",
    "        [2, 0, 0]\n",
    "    ]\n",
    "])\n",
    "\n",
    "input_.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630afe75-1ee8-421e-b8a6-999e27601481",
   "metadata": {},
   "source": [
    "Tenemos un 2 lotes de 2 observaciones cada uno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a9c646d-4087-475f-989a-54aae435cb86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3],\n",
       "         [3, 0, 0]],\n",
       "\n",
       "        [[1, 2, 0],\n",
       "         [2, 0, 0]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55850b01-f8ca-47e5-85f5-1b0a5ec315ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = embedding(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73df6c5b-7963-46ee-a701-67bf38ada364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dabedb7-2bda-49f6-888e-20c348fa1c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.5736,  0.7405],\n",
       "          [-0.7917, -0.0940],\n",
       "          [ 1.0521, -1.1056]],\n",
       "\n",
       "         [[ 1.0521, -1.1056],\n",
       "          [ 1.2157,  0.9606],\n",
       "          [ 1.2157,  0.9606]]],\n",
       "\n",
       "\n",
       "        [[[-0.5736,  0.7405],\n",
       "          [-0.7917, -0.0940],\n",
       "          [ 1.2157,  0.9606]],\n",
       "\n",
       "         [[-0.7917, -0.0940],\n",
       "          [ 1.2157,  0.9606],\n",
       "          [ 1.2157,  0.9606]]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e6f4d35-a1fd-46f9-a177-23d70b178036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c78eb8c3-9a1a-49ad-b94d-7d783468f266",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = LinearWeightedMean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0a54ba9-05b5-414b-a658-041694aea923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2593, -0.7678],\n",
       "         [ 1.0706,  0.9285]],\n",
       "\n",
       "        [[ 0.3533,  0.4193],\n",
       "         [ 1.1610,  0.8789]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = avg(embed)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de8a7fcc-e629-4733-992f-31de5383b606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "582fe5be-521f-431d-9b84-a0d7f28f6d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "wmean = WeightedMeanEmbeddingBag(num_embeddings=4, embedding_dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c89484b8-9dcb-4c65-8b07-4fbd2d577220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3720, 0.8317],\n",
       "         [0.2079, 0.2181]],\n",
       "\n",
       "        [[0.3356, 0.3721],\n",
       "         [0.1486, 0.2507]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = wmean(input_)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61bc80a9-9135-4321-988d-45fd5bfb9acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WeightedMeanEmbeddingBag(\n",
       "  (_emb): Embedding(4, 2)\n",
       "  (_avg): LinearWeightedMean(\n",
       "    (_avg): Conv1d(3, 1, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1786237-c7a3-4741-9502-78e1f9013b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
